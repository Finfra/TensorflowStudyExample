{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Tutorial - 5. RNN\n",
    "\n",
    "본 문서는 TensorFlow 를 사용하여 Deep Learning을 구현하기 위한 기초적인 실습 자료이다.\n",
    "\n",
    "The code and comments are written by Dong-Hyun Kwak <imcomking@gmail.com><br>\n",
    "Upgraed to Tensorflow v1.9 by NamJungGu <nowage@gmail.com> \n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks\n",
    "지금까지 배운 MLP, CNN과 더불어 Deep Learning에서 쓰이는 가장 강력한 모델로 Recurrent Neural Networks를 빼놓을 수 없다. 마지막으로 이 알고리즘 또한 익혀보자.\n",
    "\n",
    "\n",
    "Recurrent Neural Networks, 이하 RNN는 다음과 같은 구조를 가진 모델이다. RNN은 자기자신을 향하는 weight를 이용해 데이터간의 시간관계를 학습할 수 있다. 이러한 문제들을 시계열 학습이라고 부르며, 기존에 널리 쓰이던 Hidden Markov Model을 뉴럴넷을 이용해 구현했다고 볼 수 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"SimpleRNN01.png\">\n",
    "(출처: https://raw.githubusercontent.com/peterroelants/peterroelants.github.io/master/notebooks/RNN_implementation/img/SimpleRNN01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 구조는 1개의 Recurrent weight를 가진 hidden node이다. 이러한 hidden node들이 여러개를 모여 1개의 RNN layer를 형성하고, 이것이 다시 deep 하게 쌓이는 모델 또한 가능하다.(그러나 RNN은 deep 하게 쌓을 경우 학습이 쉽지 않다.)\n",
    "\n",
    "RNN의 경우 MLP나 CNN에 비해서 구현이 다소 복잡하다. 따라서 RNN은 skflow 라는 TensorFlow 공식 wrapping library를 활용해서 구현해보자.\n",
    "\n",
    "https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/learn/python/learn\n",
    "\n",
    "Skflow는 사용자가 최소한의 노력으로, 매우 추상화된 함수들을 사용해 deep learning 모델을 구축하고 학습할 수 있게 도와주는 역할을 한다.\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./word_embeddings_colah.png\" width='600'>\n",
    "(출처: http://sebastianruder.com/word-embeddings-1/)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "char_rdic = list('helo')  # id -> char\n",
    "char_dic = {w: i for i, w in enumerate(char_rdic)}  # char -> id\n",
    "\n",
    "x_data = np.array([\n",
    "    [1, 0, 0, 0],  # h\n",
    "    [0, 1, 0, 0],  # e\n",
    "    [0, 0, 1, 0],  # l\n",
    "    [0, 0, 1, 0],  # l\n",
    "],\n",
    "    dtype='f')\n",
    "\n",
    "sample = [char_dic[c] for c in \"hello\"]  # to index\n",
    "\n",
    "# Configuration\n",
    "char_vocab_size = len(char_dic)\n",
    "rnn_size = char_vocab_size  # 1 hot coding (one of 4)\n",
    "time_step_size = 4  # 'hell' -> predict 'ello'\n",
    "batch_size = 1  # one sample\n",
    "\n",
    "# RNN model\n",
    "#rnn_cell = tf.nn.rnn_cell.BasicRNNCell(rnn_size)\n",
    "rnn_cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
    "state = tf.zeros([batch_size, rnn_cell.state_size])\n",
    "X_split = tf.split(axis=0, num_or_size_splits=time_step_size, value=x_data)\n",
    "#outputs, state = tf.nn.rnn(rnn_cell, X_split, state)\n",
    "outputs, state = tf.contrib.rnn.static_rnn(rnn_cell, X_split, state)\n",
    "# logits: list of 2D Tensors of shape [batch_size x num_decoder_symbols]\n",
    "# targets: list of 1D batch-sized int32 Tensors of the same length as logits.\n",
    "# weights: list of 1D batch-sized float-Tensors of the same length as logits.\n",
    "logits = tf.reshape(tf.concat(axis=1, values=outputs), [-1, rnn_size])\n",
    "targets = tf.reshape(sample[1:], [-1])\n",
    "weights = tf.ones([time_step_size * batch_size])\n",
    "\n",
    "loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [targets], [weights])\n",
    "cost = tf.reduce_sum(loss) / batch_size\n",
    "train_op = tf.train.RMSPropOptimizer(0.01, 0.9).minimize(cost)\n",
    "\n",
    "# Launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    # you need to initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(100):\n",
    "        sess.run(train_op)\n",
    "        result = sess.run(tf.arg_max(logits, 1))\n",
    "        print(\"%r, %r\" % (result, [char_rdic[t] for t in result]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
