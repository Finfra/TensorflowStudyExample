{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Tutorial - 5. RNN\n",
    "\n",
    "본 문서는 TensorFlow 를 사용하여 Deep Learning을 구현하기 위한 기초적인 실습 자료이다.\n",
    "\n",
    "The code and comments are written by Dong-Hyun Kwak <imcomking@gmail.com><br>\n",
    "Upgraed to Tensorflow v1.9 by NamJungGu <nowage@gmail.com> \n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recurrent Neural Networks\n",
    "지금까지 배운 MLP, CNN과 더불어 Deep Learning에서 쓰이는 가장 강력한 모델로 Recurrent Neural Networks를 빼놓을 수 없다. 마지막으로 이 알고리즘 또한 익혀보자.\n",
    "\n",
    "\n",
    "Recurrent Neural Networks, 이하 RNN는 다음과 같은 구조를 가진 모델이다. RNN은 자기자신을 향하는 weight를 이용해 데이터간의 시간관계를 학습할 수 있다. 이러한 문제들을 시계열 학습이라고 부르며, 기존에 널리 쓰이던 Hidden Markov Model을 뉴럴넷을 이용해 구현했다고 볼 수 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"rnn_hello.jpg\">\n",
    "<center> (Source : https://mc.ai/hello-rnn/) </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 구조는 1개의 Recurrent weight를 가진 hidden node이다. 이러한 hidden node들이 여러개를 모여 1개의 RNN layer를 형성하고, 이것이 다시 deep 하게 쌓이는 모델 또한 가능하다.(그러나 RNN은 deep 하게 쌓을 경우 학습이 쉽지 않다.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "char_rdic = list('helo')  # id -> char\n",
    "char_dic = {w: i for i, w in enumerate(char_rdic)}  # char -> id\n",
    "\n",
    "x_data = np.array([\n",
    "    [1, 0, 0, 0],  # h\n",
    "    [0, 1, 0, 0],  # e\n",
    "    [0, 0, 1, 0],  # l\n",
    "    [0, 0, 1, 0],  # l\n",
    "],\n",
    "    dtype='f')\n",
    "\n",
    "sample = [char_dic[c] for c in \"hello\"]  # to index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "char_vocab_size = len(char_dic)\n",
    "rnn_size = char_vocab_size  # 1 hot coding (one of 4)\n",
    "time_step_size = 4  # 'hell' -> predict 'ello'\n",
    "batch_size = 1  # one sample\n",
    "\n",
    "# RNN model\n",
    "#rnn_cell = tf.nn.rnn_cell.BasicRNNCell(rnn_size)\n",
    "rnn_cell = tf.contrib.rnn.BasicRNNCell(rnn_size)\n",
    "state = tf.zeros([batch_size, rnn_cell.state_size])\n",
    "X_split = tf.split(axis=0, num_or_size_splits=time_step_size, value=x_data)\n",
    "#outputs, state = tf.nn.rnn(rnn_cell, X_split, state)\n",
    "outputs, state = tf.contrib.rnn.static_rnn(rnn_cell, X_split, state)\n",
    "# logits: list of 2D Tensors of shape [batch_size x num_decoder_symbols]\n",
    "# targets: list of 1D batch-sized int32 Tensors of the same length as logits.\n",
    "# weights: list of 1D batch-sized float-Tensors of the same length as logits.\n",
    "logits = tf.reshape(tf.concat(axis=1, values=outputs), [-1, rnn_size])\n",
    "targets = tf.reshape(sample[1:], [-1])\n",
    "weights = tf.ones([time_step_size * batch_size])\n",
    "\n",
    "loss = tf.contrib.legacy_seq2seq.sequence_loss_by_example([logits], [targets], [weights])\n",
    "cost = tf.reduce_sum(loss) / batch_size\n",
    "train_op = tf.train.RMSPropOptimizer(0.01, 0.9).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([3, 3, 3, 3]), ['o', 'o', 'o', 'o']\n",
      "array([3, 3, 3, 3]), ['o', 'o', 'o', 'o']\n",
      "array([3, 3, 3, 3]), ['o', 'o', 'o', 'o']\n",
      "array([3, 3, 3, 3]), ['o', 'o', 'o', 'o']\n",
      "array([3, 3, 3, 3]), ['o', 'o', 'o', 'o']\n",
      "array([3, 3, 3, 3]), ['o', 'o', 'o', 'o']\n",
      "array([3, 3, 3, 3]), ['o', 'o', 'o', 'o']\n",
      "array([3, 3, 3, 3]), ['o', 'o', 'o', 'o']\n",
      "array([3, 3, 3, 3]), ['o', 'o', 'o', 'o']\n",
      "array([3, 3, 3, 3]), ['o', 'o', 'o', 'o']\n",
      "array([3, 3, 3, 3]), ['o', 'o', 'o', 'o']\n",
      "array([3, 3, 3, 3]), ['o', 'o', 'o', 'o']\n",
      "array([3, 3, 3, 3]), ['o', 'o', 'o', 'o']\n",
      "array([3, 3, 3, 3]), ['o', 'o', 'o', 'o']\n",
      "array([3, 3, 3, 3]), ['o', 'o', 'o', 'o']\n",
      "array([3, 3, 3, 3]), ['o', 'o', 'o', 'o']\n",
      "array([3, 3, 3, 3]), ['o', 'o', 'o', 'o']\n",
      "array([3, 2, 3, 3]), ['o', 'l', 'o', 'o']\n",
      "array([3, 2, 3, 3]), ['o', 'l', 'o', 'o']\n",
      "array([3, 2, 3, 3]), ['o', 'l', 'o', 'o']\n",
      "array([3, 2, 3, 3]), ['o', 'l', 'o', 'o']\n",
      "array([3, 2, 3, 3]), ['o', 'l', 'o', 'o']\n",
      "array([3, 2, 3, 3]), ['o', 'l', 'o', 'o']\n",
      "array([3, 2, 3, 3]), ['o', 'l', 'o', 'o']\n",
      "array([3, 2, 3, 3]), ['o', 'l', 'o', 'o']\n",
      "array([3, 2, 3, 3]), ['o', 'l', 'o', 'o']\n",
      "array([2, 2, 3, 3]), ['l', 'l', 'o', 'o']\n",
      "array([2, 2, 3, 3]), ['l', 'l', 'o', 'o']\n",
      "array([1, 2, 3, 3]), ['e', 'l', 'o', 'o']\n",
      "array([1, 2, 3, 3]), ['e', 'l', 'o', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n",
      "array([1, 2, 2, 3]), ['e', 'l', 'l', 'o']\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph in a session\n",
    "with tf.Session() as sess:\n",
    "    # you need to initialize all variables\n",
    "    tf.global_variables_initializer().run()\n",
    "    for i in range(100):\n",
    "        sess.run(train_op)\n",
    "        result = sess.run(tf.arg_max(logits, 1))\n",
    "        print(\"%r, %r\" % (result, [char_rdic[t] for t in result]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
