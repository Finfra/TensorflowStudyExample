{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Tutorial - 6. CNN\n",
    "\n",
    "본 문서는 TensorFlow 를 사용하여 Deep Learning을 구현하기 위한 기초적인 실습 자료이다.\n",
    "\n",
    "The code and comments are written by Dong-Hyun Kwak <imcomking@gmail.com><br>\n",
    "Upgraed to Tensorflow v1.10 by NamJungGu <nowage@gmail.com> \n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc-sa/4.0/\">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Convolutional Neural Networks\n",
    "이번에는 이미지 인식 분야에서 가장 성공적으로 쓰이고 있는 Convolutional Neural Networks를 실습해본다.\n",
    "\n",
    "Convolutional Neural Networks, 이하 CNN은 아래와 같은 Convolutional Layer를 여러층 가진 딥러닝 모델을 뜻한다.\n",
    "\n",
    "![](http://ufldl.stanford.edu/tutorial/images/Cnn_layer.png)\n",
    "(출처: http://ufldl.stanford.edu/tutorial/images/Cnn_layer.png)\n",
    "\n",
    "\n",
    "\n",
    "이번에는 간단한 구조를 가진 CNN을 구현하고 방금전에 사용했던 MNIST 데이터를 학습시켜 보고, MLP와의 성능 차이를 비교해본다.\n",
    "\n",
    "아래의 코드를 보면 MLP와 전체 구조는 매우 유사한데, 중간에 Convolutional을 비롯해 처음 보는 여러 연산들이 추가 된 것을 알 수 있다. 또한 CNN을 효과적으로 학습하기 위해서는 Weight의 초기화를 0으로 하는 것이 아니라, 랜덤으로 해주어야하는데, 여기서는 간단히 가우시안을 이용하여 초기화 하였다. 그밖에 dropout과 relu 등이 사용되었다.\n",
    "\n",
    "각 함수와 연산들의 자세한 설명은 아래 코드를 보면서 하나하나 분석해보자.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('./MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholder is used for feeding data.\n",
    "x = tf.placeholder(\"float\", shape=[None, 784], name = 'x') # none represents variable length of dimension. 784 is the dimension of MNIST data.\n",
    "y_target = tf.placeholder(\"float\", shape=[None, 10], name = 'y_target') # shape argument is optional, but this is useful to debug.\n",
    "\n",
    "\n",
    "\n",
    "# reshape input data\n",
    "x_image = tf.reshape(x, [-1,28,28,1], name=\"x_image\")\n",
    "\n",
    "# Build a convolutional layer and maxpooling with random initialization\n",
    "W_conv1 = tf.Variable(tf.truncated_normal([5, 5, 1, 32], stddev=0.1), name=\"W_conv1\") # W is [row, col, channel, feature]\n",
    "b_conv1 = tf.Variable(tf.zeros([32]), name=\"b_conv1\")\n",
    "h_conv1 = tf.nn.relu(tf.nn.conv2d(x_image, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1, name=\"h_conv1\")\n",
    "h_pool1 = tf.nn.max_pool( h_conv1 , ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name = \"h_pool1\")\n",
    "\n",
    "# Repeat again with 64 number of filters\n",
    "W_conv2 = tf.Variable(tf.truncated_normal([5, 5, 32, 64], stddev=0.1), name=\"W_conv2\") # W is [row, col, channel, feature]\n",
    "b_conv2 = tf.Variable(tf.zeros([64]), name=\"b_conv2\")\n",
    "h_conv2 = tf.nn.relu(tf.nn.conv2d(h_pool1, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2, name=\"h_conv2\")\n",
    "h_pool2 = tf.nn.max_pool( h_conv2 , ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME', name = \"h_pool2\")\n",
    "\n",
    "# Build a fully connected layer\n",
    "h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*64], name=\"h_pool2_flat\")\n",
    "W_fc1 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1024], stddev=0.1), name = 'W_fc1')\n",
    "b_fc1 = tf.Variable(tf.zeros([1024]), name = 'b_fc1')\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1, name=\"h_fc1\")\n",
    "\n",
    "\n",
    "# Dropout Layer\n",
    "keep_prob = tf.placeholder(\"float\", name=\"keep_prob\")\n",
    "h_fc1 = tf.nn.dropout(h_fc1, keep_prob, name=\"h_fc1_drop\")\n",
    "\n",
    "# Build a fully connected layer with softmax \n",
    "W_fc2 = tf.Variable(tf.truncated_normal([1024, 10], stddev=0.1), name = 'W_fc2')\n",
    "b_fc2 = tf.Variable(tf.zeros([10]), name = 'b_fc2')\n",
    "y=tf.nn.softmax(tf.matmul(h_fc1, W_fc2) + b_fc2, name=\"y\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# define the Loss function\n",
    "cross_entropy = -tf.reduce_sum(y_target*tf.log(y), name = 'cross_entropy')\n",
    "\n",
    "\n",
    "# define optimization algorithm\n",
    "#train_step = tf.train.GradientDescentOptimizer(0.01).minimize(cross_entropy)\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_target, 1))\n",
    "# correct_prediction is list of boolean which is the result of comparing(model prediction , data)\n",
    "\n",
    "\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\")) \n",
    "# tf.cast() : changes true -> 1 / false -> 0\n",
    "# tf.reduce_mean() : calculate the mean\n",
    "\n",
    "\n",
    "\n",
    "# create summary of parameters\n",
    "tf.summary.histogram('weights_1', W_conv1)\n",
    "tf.summary.histogram('weights_2', W_conv2)\n",
    "tf.summary.histogram('y', y)\n",
    "tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "merged = tf.summary.merge_all()\n",
    "summary_writer = tf.summary.FileWriter(\"summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Session\n",
    "with tf.Session(config=tf.ConfigProto(gpu_options=tf.GPUOptions(allow_growth =True))) as sess:  # open a session which is a envrionment of computation graph.\n",
    "    sess.run(tf.global_variables_initializer())# initialize the variables\n",
    "\n",
    "\n",
    "    # training the MLP\n",
    "    for i in range(5001): # minibatch iteraction\n",
    "        batch = mnist.train.next_batch(100) # minibatch size\n",
    "        sess.run(train_step, feed_dict={x: batch[0], y_target: batch[1], keep_prob: 0.5}) # placeholder's none length is replaced by i:i+100 indexes\n",
    "\n",
    "        if i%500 == 0:\n",
    "            train_accuracy = sess.run(accuracy, feed_dict={x:batch[0], y_target: batch[1], keep_prob: 1})\n",
    "            print (\"step %d, training accuracy: %.3f\"%(i, train_accuracy))\n",
    "\n",
    "            # calculate the summary and write.\n",
    "            summary = sess.run(merged, feed_dict={x:batch[0], y_target: batch[1], keep_prob: 1})\n",
    "            summary_writer.add_summary(summary , i)\n",
    "\n",
    "    # for given x, y_target data set\n",
    "    print  (\"test accuracy: %g\"% sess.run(accuracy, feed_dict={x: mnist.test.images[0:250], y_target: mnist.test.labels[0:250], keep_prob: 1}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "학습 결과, MLP보다 훨씬 성능이 높아진 것을 알 수 있고 동시에 학습 속도가 MLP보다 느려진 것을 확인할 수 있다. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
